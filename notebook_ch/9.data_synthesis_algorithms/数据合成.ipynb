{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、背景介绍\n",
    "\n",
    "深度学习方法的性能与训练数据的质量和数量密切相关，但是获取大量带标注的数据已经成为许多深度学习任务高效开发的瓶颈。OCR识别任务不同场景的图片都有独特的风格，比如字体、模糊、光照等，收集足够的数据是一项具有挑战且昂贵的任务，而且手工标注是非常耗时、容易出现人为错误。所以在无法获取足够多标注数据的情况下，自动合成带标注的图像受到越来越多的关注，成功缓解了数据缺少和手工标注数据的问题。以OCR为例，合成数据对于训练OCR文本检测和识别模型非常重要，而且在众多算法中验证是有效的。在本节，将介绍近几年来一些具有代表性的数据合成方法，如 SynthText、Verisimilar、SynthText3D、SF-GAN、SRNet、ScrabbleGAN、UnrealText。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据合成算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ankush Gupta等人(2016)[1]提出了一种新的合成文本图像的方法SynthText，该方法将合成文本自然地覆盖到现有背景图像上，生成自然场景下的文字图像。SynthText合成文本图像的流程包含以下内容：1）首先获取大量的无文字背景图片、字体以及文本语料；2）根据局部的颜色和纹理将图像分割为连续的区域，并使用CNN获取像素级的深度信息；3）然后，根据语义信息和深度信息进行筛选，获得候选区域；4）接下来，根据候选区域的颜色选择文本及其轮廓(可选)的颜色；5）最后，使用随机选择的字体渲染文本，渲染图像效果如 **图1** 所示。\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/b917b9df9e4040a5befffa9af0144d958d60d3835cfe47848b08be8066bea0af' width='500'></center>\n",
    "<center>图1 SynthText合成图像效果</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fangneng Zhan等人(2018)[2]提出一种新颖的Verisimilar图像合成技术，旨在生成大量带注释的场景文本图像，用于训练准确、鲁棒的场景文本检测和识别模型。作者结合语义分割和显著性进行文本合成，引入语义分割使得文字只出现在一个可感知的物体上，例如场景文本往往出现在墙壁或桌子表面，而不是食物或植物叶子。它利用视觉显著性来确定文本嵌入位置，使得文本和背景区域能够明显区分开。最后通过自适应地学习真实场景文本图像的特征，确定嵌入文本的颜色和亮度。该方法在训练准确、鲁棒的场景文本检测与识别模型方面具有良好的性能，实现了逼真的场景文本图像合成，图像效果如 **图2** 所示。\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/2df29d75a9e34cb1977ec3e978cc764a1d219dd433244cde9d0bfeaf174ab31e' width='500'></center>\n",
    "<center>图2 Verisimilar合成图像效果</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Minghui Liao等人(2019)[3]提出了从三维空间合成场景文本图像的模型SynthText3D，可以在生成的文本图像中实现复杂的透视变换、各种光照条件和遮挡。具体而言，各种字体的文本首先被嵌入到三维空间中合适的位置。然后在三维空间中渲染不同光照和遮挡等条件的虚拟场景，渲染后文本和背景融为一体。最后，作者设置了不同位置和方向的相机，可以产生出具有不同视角的相同文本图像。SynthText3D生成图像效果如 **图3** 所示。\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/20a2556632e34f76a1ea953d0c433c76918de2d9bd3346d794cf064592320a99' width='500'></center>\n",
    "<center>图3 SynthText3D合成图像效果</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fangneng Zhan等人(2019)[4]提出一种Spatial Fusion GAN(SF-GAN)模型，结合了几何生成器和外观生成器，生成几何和外观空间都近似真实的图像。几何生成器学习背景图像的上下文内容将前景对象放置到背景图像中。外观生成器调整前景物体的颜色、亮度和样式。同时作者设计了一个融合网络，该网络引入了保留细节的导向滤波，以保证外观的真实感，合成图像效果如 **图4** 所示。\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/deeca0de27bb423ba282695d3a1005bd9236c1cf2b694a7492a0eb92a0eed5c6' width='500'></center>\n",
    "<center>图4 SF-GAN合成图像效果</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Liang Wu等人(2019)[5]提出了一个端到端可训练网络style retention network（SRNet），在保持原始图像真实感的同时，将原始图像中的词替换或修改为另一个词，即同时保留背景和文本的风格。Style-Text主要框架包括：1)文本前景风格迁移模块；2）背景抽取模块；3）融合模块。文本转换模块将源图像的文本内容更换为目标文本，同时保持原始文本风格。背景修复模块擦出源图像中的文本，并用适当的纹理填充文本区域。融合模块将前两块的信息结合起来，生成编辑后的文本图像。经过这样三步，就可以迅速实现图像文本风格迁移。合成图像效果如 **图5** 所示。\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/34d77b97c31f4beebd0509c5e080f38f38a8da1501ff4a168eee11a2e22bff0a' width='500'></center>\n",
    "<center>图5 SRNet合成图像效果</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sharon Fogel等人(2020)[6]提出一种半监督的方法ScrabbleGAN来合成手写本文图像，半监督的方法除了使用标注数据外，还允许使用未标注数据来训练合成手写文本图像模型。ScrabbleGAN依赖于一种新颖的全卷积生成模型，它可以生成任意长度单词的图像，甚至可以生成完整的句子图像。此外，ScrabbleGAN的生成器还可以控制生成的文本样式，如文本是否是草书、笔画的粗细等，生成手写数字图像效果如 **图6** 所示。\n",
    "\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/c766196274454a589767eda0fb5225336d0c8fd2aa2c43c4a502cefc0fdf63cc' width='500'>\n",
    "</center><center>图6 ScrabbleGAN合成图像效果</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Shangbang Long等人(2020)[7]提出一种有效的图像合成方法UnrealText，从三维空间中合成场景文本图像。UnrealText是基于著名的Unreal Engine 4(UE4)引擎构建的，因此被命名为UnrealText。具体而言，文本实例被视为前景带纹理的平面多边形网格，这些网格被置于三维空间中合适的位置。经过渲染后，文本与场景融为一个整体，在光照、遮挡和透视变换上实现逼真的视觉效果，生成图像效果图如 **图7** 所示。UnrealText引擎具有逼真的渲染效果和较高的可扩展性，显著提高了文本检测和生成模型的性能。同时构建了大规模多语言场景文本数据集，为进一步研究提供了帮助。\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/1d9bbd11a4a441d4be3c72747f65cb11f67f7a702f6b4f85a4eb16670855a9b1' width='500'></center>\n",
    "<center>图7 UnrealText合成图像效果</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、总结\n",
    "\n",
    "文本检测和识别领域公开数据集可能不能满足当前使用场景或数据集比较小，而人工标注数据又耗时耗力，合成OCR数据已经成为一个普遍做法。本节我们总结了一些数据合成方法，如SynthText、SynthText3D、SF-GAN、SRNet等，通过这些方法生成OCR数据，提升文本检测和识别的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 参考文献\n",
    "[1] Gupta A, Vedaldi A, Zisserman A. Synthetic data for text localisation in natural images[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2315-2324.\n",
    "\n",
    "[2] Zhan F, Lu S, Xue C. Verisimilar image synthesis for accurate detection and recognition of texts in scenes[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 249-266.\n",
    "\n",
    "[3] Liao M, Song B, Long S, et al. SynthText3D: synthesizing scene text images from 3D virtual worlds[J]. Science China Information Sciences, 2020, 63(2): 1-14.\n",
    "\n",
    "[4] Zhan F, Zhu H, Lu S. Spatial fusion gan for image synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 3653-3662.\n",
    "\n",
    "[5] Wu L, Zhang C, Liu J, et al. Editing text in the wild[C]//Proceedings of the 27th ACM international conference on multimedia. 2019: 1500-1508\n",
    "\n",
    "[6] Fogel S, Averbuch-Elor H, Cohen S, et al. Scrabblegan: Semi-supervised varying length handwritten text generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 4324-4333.\n",
    "\n",
    "[7] Long S, Yao C. Unrealtext: Synthesizing realistic scene text images from the unreal world[J]. arXiv preprint arXiv:2003.10608, 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
